sig <- backTransform(linearComb(hnDay['det'], c(1, i+157)))@estimate
ea <- 2*pi * integrate(grhn, 0, 400, sigma=sig)$value # effective area
er <- sqrt(ea / pi) # effective radius
p <- ea / (pi*400^2) #detection probability
out <- c(p = p , er = er)
}
sig[[i,5]] <- quantile(parboot(hnDay,getPcovsL,nsim=10,report=10)@t.star[,1],probs = 0.025)
}
View(sig)
## Loop function for calculating P-hat across days of the season
sig <- matrix(nrow = 33, ncol = 6)
colnames(sig) <- c("sigma", "ea", "er", "p","lowerCI", "upperCI")
for (i in 1:33) {
sig[[i,1]] <- backTransform(linearComb(hnDay['det'], c(1, i+157)))@estimate # this is a kludge to index days (i.e., lowest value = day 158)
sig[[i,2]] <- 2*pi * integrate(grhn, 0, 400, sigma=sig[[i,1]])$value # effective area
sig[[i,3]] <- sqrt(sig[[i,2]] / pi) # effective radius
sig[[i,4]] <- sig[[i,2]] / (pi*400^2) #detection probability
getPcovsL <- function(hnDay) {
sig <- backTransform(linearComb(hnDay['det'], c(1, i+157)))@estimate
ea <- 2*pi * integrate(grhn, 0, 400, sigma=sig)$value # effective area
er <- sqrt(ea / pi) # effective radius
p <- ea / (pi*400^2) #detection probability
out <- c(p = p , er = er)
}
sig[[i,5]] <- quantile(parboot(hnDay,getPcovsL, nsim = 10, report = 10)@t.star[,1],probs = 0.025)
sig[[i,6]] <- quantile(parboot(hnDay,getPcovsL, nsim = 10, report = 10)@t.star[,1], probs = 0.975)
}
View(sig)
rm(bsEst)
rm(sig)
## Loop function for calculating P-hat across days of the season
distanceCovPred <- matrix(nrow = 33, ncol = 6)
colnames(distanceCovPred) <- c("sigma", "ea", "er", "p","lowerCI", "upperCI")
for (i in 1:33) {
distanceCovPred[[i,1]] <- backTransform(linearComb(hnDay['det'], c(1, i+157)))@estimate # this is a kludge to index days (i.e., lowest value = day 158)
distanceCovPred[[i,2]] <- 2*pi * integrate(grhn, 0, 400, sigma=distanceCovPred[[i,1]])$value # effective area
distanceCovPred[[i,3]] <- sqrt(distanceCovPred[[i,2]] / pi) # effective radius
distanceCovPred[[i,4]] <- distanceCovPred[[i,2]] / (pi*400^2) #detection probability
getPcovsL <- function(hnDay) {
sig <- backTransform(linearComb(hnDay['det'], c(1, i+157)))@estimate
ea <- 2*pi * integrate(grhn, 0, 400, sigma=sig)$value # effective area
er <- sqrt(ea / pi) # effective radius
p <- ea / (pi*400^2) #detection probability
out <- c(p = p , er = er)
}
distanceCovPred[[i,5]] <- quantile(parboot(hnDay,getPcovsL, nsim = 10, report = 10)@t.star[,1],probs = 0.025)
distanceCovPred[[i,6]] <- quantile(parboot(hnDay,getPcovsL, nsim = 10, report = 10)@t.star[,1], probs = 0.975)
}
View(distanceCovPred)
## Loop function for calculating P-hat across days of the season
distanceCovPred <- matrix(nrow = 33, ncol = 6) #create a matrix for output
colnames(distanceCovPred) <- c("sigma", "ea", "er", "p","lowerCI", "upperCI") #give the columns names
for (i in 1:33) { #this loops through 33 days of the year to generate estimates of p-hat
distanceCovPred[[i,1]] <- backTransform(linearComb(hnDay['det'], c(1, i+157)))@estimate # this is a kludge to index days (i.e., lowest value = day 158)
distanceCovPred[[i,2]] <- 2*pi * integrate(grhn, 0, 400, sigma=distanceCovPred[[i,1]])$value # effective area
distanceCovPred[[i,3]] <- sqrt(distanceCovPred[[i,2]] / pi) # effective radius
distanceCovPred[[i,4]] <- distanceCovPred[[i,2]] / (pi*400^2) #detection probability
getPcovsL <- function(hnDay) { #this defines the function that 'parboot' will use
sig <- backTransform(linearComb(hnDay['det'], c(1, i+157)))@estimate
ea <- 2*pi * integrate(grhn, 0, 400, sigma=sig)$value # effective area
er <- sqrt(ea / pi) # effective radius
p <- ea / (pi*400^2) #detection probability
out <- c(p = p , er = er)
} #following lines are for pulling out upper and lower 95% Ci from bootstrap
distanceCovPred[[i,5]] <- quantile(parboot(hnDay,getPcovsL, nsim = 100, report = 100)@t.star[,1],probs = 0.025)
distanceCovPred[[i,6]] <- quantile(parboot(hnDay,getPcovsL, nsim = 100, report = 100)@t.star[,1], probs = 0.975)
}
View(distanceHNDaySTAN)
for (i in 1:33) { #this loops through 33 days of the year to generate estimates of p-hat
distanceCovPred[[i,1]] <- backTransform(linearComb(hnDay['det'], c(1, i+157)))@estimate # this is a kludge to index days (i.e., lowest value = day 158)
distanceCovPred[[i,2]] <- 2*pi * integrate(grhn, 0, 400, sigma=distanceCovPred[[i,1]])$value # effective area
distanceCovPred[[i,3]] <- sqrt(distanceCovPred[[i,2]] / pi) # effective radius
distanceCovPred[[i,4]] <- distanceCovPred[[i,2]] / (pi*400^2) #detection probability
getPcovsL <- function(hnDay) { #this defines the function that 'parboot' will use
sig <- backTransform(linearComb(hnDay['det'], c(1, i+157)))@estimate
ea <- 2*pi * integrate(grhn, 0, 400, sigma=sig)$value # effective area
er <- sqrt(ea / pi) # effective radius
p <- ea / (pi*400^2) #detection probability
out <- c(p = p , er = er)
} #following lines are for pulling out upper and lower 95% Ci from bootstrap
distanceCovPred[[i,5]] <- quantile(parboot(hnDay,getPcovsL, nsim = 10, report = 10)@t.star[,1],probs = 0.025)
distanceCovPred[[i,6]] <- quantile(parboot(hnDay,getPcovsL, nsim = 10, report = 10)@t.star[,1], probs = 0.975)
}
for (i in 1:33) { #this loops through 33 days of the year to generate estimates of p-hat
distanceCovPred[[i,1]] <- backTransform(linearComb(hnDay['det'], c(1, i+157)))@estimate # this is a kludge to index days (i.e., lowest value = day 158)
distanceCovPred[[i,2]] <- 2*pi * integrate(grhn, 0, 400, sigma=distanceCovPred[[i,1]])$value # effective area
distanceCovPred[[i,3]] <- sqrt(distanceCovPred[[i,2]] / pi) # effective radius
distanceCovPred[[i,4]] <- distanceCovPred[[i,2]] / (pi*400^2) #detection probability
getPcovsL <- function(hnDay) { #this defines the function that 'parboot' will use
sig <- backTransform(linearComb(hnDay['det'], c(1, i+157)))@estimate
ea <- 2*pi * integrate(grhn, 0, 400, sigma=sig)$value # effective area
er <- sqrt(ea / pi) # effective radius
p <- ea / (pi*400^2) #detection probability
out <- c(p = p , er = er)
} #following lines are for pulling out upper and lower 95% Ci from bootstrap
distanceCovPred[[i,5]] <- quantile(parboot(hnDay,getPcovsL, nsim = 100, report = 10)@t.star[,1],probs = 0.025)
distanceCovPred[[i,6]] <- quantile(parboot(hnDay,getPcovsL, nsim = 100, report = 10)@t.star[,1], probs = 0.975)
}
for (i in 1:33) { #this loops through 33 days of the year to generate estimates of p-hat
distanceCovPred[[i,1]] <- backTransform(linearComb(hnDay['det'], c(1, i+157)))@estimate # this is a kludge to index days (i.e., lowest value = day 158)
distanceCovPred[[i,2]] <- 2*pi * integrate(grhn, 0, 400, sigma=distanceCovPred[[i,1]])$value # effective area
distanceCovPred[[i,3]] <- sqrt(distanceCovPred[[i,2]] / pi) # effective radius
distanceCovPred[[i,4]] <- distanceCovPred[[i,2]] / (pi*400^2) #detection probability
getPcovsL <- function(hnDay) { #this defines the function that 'parboot' will use
sig <- backTransform(linearComb(hnDay['det'], c(1, i+157)))@estimate
ea <- 2*pi * integrate(grhn, 0, 400, sigma=sig)$value # effective area
er <- sqrt(ea / pi) # effective radius
p <- ea / (pi*400^2) #detection probability
out <- c(p = p , er = er)
} #following lines are for pulling out upper and lower 95% Ci from bootstrap
distanceCovPred[[i,5]] <- quantile(parboot(hnDay,getPcovsL, nsim = 100, report = 100, ncores = 1)@t.star[,1],probs = 0.025)
distanceCovPred[[i,6]] <- quantile(parboot(hnDay,getPcovsL, nsim = 100, report = 100, ncores = 1)@t.star[,1], probs = 0.975)
}
View(distanceCovPred)
predict(hnDay, data = dsPredictionFrame, type = "det")
rm(list = c(dsPredictionFrame))
rm(dsPredictionFrame)
ggplot(data = distanceCovPred) +
geom_ribbon(aes(ymin = lowerCI, ymax = upperCI), fill = "grey80", alpha = 0.5) +
geom_line(aes(x = seq(158:181, by =1), y = p), color = "black") +
xlab("Day of year") + ylab ("Probability of detection") +
theme_bw()
ggplot(data = as.data.frame(distanceCovPred)) +
geom_ribbon(aes(ymin = lowerCI, ymax = upperCI), fill = "grey80", alpha = 0.5) +
geom_line(aes(x = seq(158:181, by =1), y = p), color = "black") +
xlab("Day of year") + ylab ("Probability of detection") +
theme_bw()
distanceCovPred[,7] <- seq(158, 181, 1)
distanceCovPred[,7] <- cbind(seq(158, 181, 1))
daysForMatrix <- seq(158, 181, 1)
max(covs$dayOfYear)
min(covs$dayOfYear)
181-158
## Predict seasonal changes in P-hat
## Loop function for calculating P-hat across days of the season
distanceCovPred <- matrix(nrow = 24, ncol = 7) #create a matrix for output
colnames(distanceCovPred) <- c("sigma", "ea", "er", "p","lowerCI", "upperCI", "Day") #give the columns names
for (i in 1:24) { #this loops through 24 days of the year to generate estimates of p-hat
distanceCovPred[[i,1]] <- backTransform(linearComb(hnDay['det'], c(1, i+157)))@estimate # this is a kludge to index days (i.e., lowest value = day 158)
distanceCovPred[[i,2]] <- 2*pi * integrate(grhn, 0, 400, sigma=distanceCovPred[[i,1]])$value # effective area
distanceCovPred[[i,3]] <- sqrt(distanceCovPred[[i,2]] / pi) # effective radius
distanceCovPred[[i,4]] <- distanceCovPred[[i,2]] / (pi*400^2) #detection probability
getPcovsL <- function(hnDay) { #this defines the function that 'parboot' will use
sig <- backTransform(linearComb(hnDay['det'], c(1, i+157)))@estimate
ea <- 2*pi * integrate(grhn, 0, 400, sigma=sig)$value # effective area
er <- sqrt(ea / pi) # effective radius
p <- ea / (pi*400^2) #detection probability
out <- c(p = p , er = er)
} #following lines are for pulling out upper and lower 95% Ci from bootstrap; parallel process didn't work, so had to set cores = 1
distanceCovPred[[i,5]] <- quantile(parboot(hnDay,getPcovsL, nsim = 100, report = 100, ncores = 1)@t.star[,1],probs = 0.025)
distanceCovPred[[i,6]] <- quantile(parboot(hnDay,getPcovsL, nsim = 100, report = 100, ncores = 1)@t.star[,1], probs = 0.975)
}
distanceCovPred[,7]<- seq(158, 181, 1)
View(distanceCovPred)
ggplot(data = as.data.frame(distanceCovPred)) +
geom_ribbon(aes(ymin = lowerCI, ymax = upperCI), fill = "grey80", alpha = 0.5) +
geom_line(aes(x = Day, y = p), color = "black") +
xlab("Day of year") + ylab ("Probability of detection") +
theme_bw()
ggplot(data = as.data.frame(distanceCovPred, x = Day, y = p)) +
geom_ribbon(aes(ymin = lowerCI, ymax = upperCI), fill = "grey80", alpha = 0.5) +
geom_line(aes(x = Day, y = p), color = "black") +
xlab("Day of year") + ylab ("Probability of detection") +
theme_bw()
ggplot(data = as.data.frame(distanceCovPred), aes(x = Day, y = p)) +
geom_ribbon(aes(ymin = lowerCI, ymax = upperCI), fill = "grey80", alpha = 0.5) +
geom_line(aes(x = Day, y = p), color = "black") +
xlab("Day of year") + ylab ("Probability of detection") +
theme_bw()
ggplot(data = removalDaySTANframe$data, aes(x = covariate, y = mn)) +
geom_ribbon(aes(ymin = lower, ymax = upper), fill = "grey80", alpha = 0.5) +
geom_line(aes(x = covariate, y = mn), color = "black") +
xlab("Day of year") + ylab ("Availability for detection") +
theme_bw()
aictab(cand.set = fmRemovalList, second.ord = T, sort = T)
## Distance removal with day-of-year as covariate on distance:
drDay <- gdistremoval(lambdaformula = ~1, phiformula = ~1, removalformula = ~dayOfYear,
distanceformula = ~dayOfYear, data = umfDR, keyfun = "halfnorm",
output = "density", unitsOut = "kmsq", mixture = "ZIP")
summary(drDay)
getPD <- function(x) {
d <- backTransform(drDay, type = "rem")
sig <- backTransform(linearComb(drDay,c(1,0), type = "dist"))
ea <- 2*pi * integrate(grhn, 0, 400, sigma=sig@estimate)$value # effective area
er <- sqrt(ea / pi) # effective radius
p <- ea / (pi*400^2) #detection probability
Pd <- p*(d@estimate)
return(Pd)
}
getPD(drDay)
backTransform(drDay, type = "rem")
backTransform(linearComb(drDay,c(1,0,0), type = "dist"))
backTransform(linearComb(drDay,c(1,0), type = "dist"))
backTransform(linearComb(drDay,c(1,0), type = "rem"))
getPD <- function(x) {
d <- backTransform(linearComb(drDay,c(1,0), type = "rem"))
sig <- backTransform(linearComb(drDay,c(1,0), type = "dist"))
ea <- 2*pi * integrate(grhn, 0, 400, sigma=sig@estimate)$value # effective area
er <- sqrt(ea / pi) # effective radius
p <- ea / (pi*400^2) #detection probability
Pd <- p*(d@estimate)
return(Pd)
}
getPD(drDay)
library(tidyverse)
library(readxl)
library(lubridate)
library(suncalc)
library(ggpmisc)
library(unmarked)
library(AICcmodavg)
library(ubms)
install.packages("ubms")
library(tidyverse)
library(readxl)
library(lubridate)
library(suncalc)
library(ggpmisc)
library(unmarked)
library(AICcmodavg)
library(ubms)
# Review and organize data, changing formats and variable names as needed.
#surveyData <- read_xlsx("~/Documents/GitHub/hornedLarks/WV_SurveyOutput.xlsx")
surveyData <- read_csv("~/Documents/GitHub/hornedLarks/WV_SHLA_22.csv")
names(surveyData)[1] <- 'surveyEvent'
#surveyData$Count_Date <- mdy(surveyData$Count_Date)
surveyData$Count_Date <- mdy(surveyData$Survey_Date)
#surveyData$Start_Time <- hms(surveyData$Start_Time)
surveyData$Start_Time <- hms(surveyData$Survey_Time)
surveyData$Site_ID <- factor(surveyData$Site_ID)
surveyData$Observer <- factor(surveyData$Observer)
surveyData$Sky_Code <- factor(surveyData$Sky_Code, levels = c("0","1","2","3","4"),
labels = c("Clear","Partly cloudy","Mostly cloudy","Fog or smoke","Drizzle"))
surveyData$Sex <- factor(surveyData$Sex, levels = c("M","F","U"), labels = c("Male","Female","Unknown"))
surveyData$Age <- factor(surveyData$Age, levels = c("A", "J"), labels = c("Adult", "Juvenile"))
surveyData$`Distance Band` <- factor(surveyData$`Distance Band`)
names(surveyData)[19] <- 'distanceBand'
surveyData$Interval_1 <- ifelse(surveyData$Interval_1 == "X", NA, surveyData$Interval_1)
surveyData$Interval_2 <- ifelse(surveyData$Interval_2 == "X", NA, surveyData$Interval_2)
surveyData$Interval_3 <- ifelse(surveyData$Interval_3 == "X", NA, surveyData$Interval_3)
surveyData$Interval_4 <- ifelse(surveyData$Interval_4 == "X", NA, surveyData$Interval_4)
surveyData$Interval_1 <- factor(surveyData$Interval_1, levels = c("C","S","V"), labels =
c("Calling", "Singing", "Visual"))
surveyData$Interval_2 <- factor(surveyData$Interval_2, levels = c("C","S","V"), labels =
c("Calling", "Singing", "Visual"))
surveyData$Interval_3 <- factor(surveyData$Interval_3, levels = c("C","S","V"), labels =
c("Calling", "Singing", "Visual"))
surveyData$Interval_4 <- factor(surveyData$Interval_4, levels = c("C","S","V"), labels =
c("Calling", "Singing", "Visual"))
surveyData$dayOfYear <- yday(surveyData$Count_Date) # create a day-of-year variable for analysis
## Add a "first detected by..." column to survey data:
surveyData <-
surveyData %>%
filter(!is.na(Sex)) %>%
select(Sex, Lark_ID, Interval_1, Interval_2, Interval_3, Interval_4) %>%
pivot_longer(., cols = 3:6, names_to = "interval", values_to = "detection") %>%
group_by(Lark_ID, Sex) %>%
filter(!is.na(detection)) %>%
summarise(firstDet = first(detection)) %>%
right_join(., surveyData, by = 'Lark_ID', keep = F) %>%
select(!(Sex.x)) %>%
rename(Sex = Sex.y)
## Add a "first detected interval" column
surveyData <-
surveyData %>%
filter(!is.na(Sex)) %>%
select(Sex, Lark_ID, Interval_1, Interval_2, Interval_3, Interval_4) %>%
pivot_longer(., cols = 3:6, names_to = "interval", values_to = "detection") %>%
group_by(Lark_ID, Sex) %>%
filter(!is.na(detection)) %>%
summarise(firstDet = first(interval)) %>%
mutate(firstInterval = ifelse(firstDet == "Interval_1",1,
ifelse(firstDet == "Interval_2", 2,
ifelse(firstDet == "Interval_3", 3,
ifelse(firstDet == "Interval_4", 4, NA))))) %>%
right_join(., surveyData, by = 'Lark_ID', keep = F) %>%
select(!(Sex.x)) %>%
select(!firstDet.x) %>%
rename(firstDet = firstDet.y, Sex = Sex.y)
# Get sunrise times to look at effect of survey timing in detections,
# here using the Corvallis airport as the location. We could calculate
# sunrise based on the lat/long of each survey point, but they are
# all going to have roughly the same sunrise so for ease I'm simply estimating
# sunrise at a single, central location.
# calculate sunrise times at Corvallis Airport on each survey date.
sunriseTimes <- getSunlightTimes(date = surveyData$Count_Date, lat = 44.50, lon = -123.28,
keep = c("sunrise"), tz="America/Los_Angeles")
#create a new variable in surveyData that has sunrise matched to the survey point.
#careful, here, because there is no matching function (i.e., this only works if the
#two data frames are sorted in the same order. This should be the case unless you
#sort one after calling this function).
surveyData$sunrise <- sunriseTimes$sunrise
#subtract the two times to get decimal hours after sunrise.
surveyData$mas <- 60*((surveyData$Start_Time@hour + surveyData$Start_Time@minute/60) -
(hour(surveyData$sunrise) + minute(surveyData$sunrise)/60))
#clean up
rm(sunriseTimes)
# Read in survey location and habitat data
habitatData <- read_csv("tbl_survey_locations_exported_09_26_2022_pct_suitable_2021.csv")
habitatData$Site_ID <- factor(habitatData$Site_ID)
habitatAndSurveyData <-
habitatData %>%
select(Site_ID,ln_UTM1083,lt_UTM1083,ln_WGS84,lt_WGS84, pct_suitable_2021) %>%
right_join(.,surveyData, by = 'Site_ID', keep = F)
habitatAndSurveyData$Site_ID <- factor(habitatAndSurveyData$Site_ID)
habitatAndSurveyData$Observer <- factor(habitatAndSurveyData$Observer)
# Make encounter histories for detected birds across intervals
surveyData$encounterHistory <- paste(if_else(surveyData$Interval_1 == "None", 0,1),
if_else(surveyData$Interval_2 == "None", 0,1),
if_else(surveyData$Interval_3 == "None", 0,1),
if_else(surveyData$Interval_4 == "None", 0,1),
sep = "")
## Create a new variable called 'distance', which translates the distance_band information into the midpoint of the distance.
## Then remove all other variables.
## THIS EXCLUDES ALL NON-SINGING MALES ##
dists <-
surveyData %>%
group_by(Site_ID) %>%
mutate(distance = ifelse(distanceBand == 1, 12.5,
ifelse(distanceBand == 2, 61,
ifelse(distanceBand == 3, 150,
ifelse(distanceBand == 4, 300, NA))))) %>%
select(Site_ID, distance, Sex, firstDet) %>%
filter(!is.na(distance), firstDet == "Singing", Sex == "Male")
## Note here that we need the "as.data.frame" argument because 'dists' is a tidyverse tibble,
## and unmarked doesn't seem to like tibbles. This forces it into a standard R data frame.
yDat <- formatDistData(distData = as.data.frame(dists), distCol = "distance", transectNameCol = "Site_ID",
dist.breaks = c(0,25,100,200,400))
## Create a data frame of site-level covariates.
covs <-
surveyData %>%
group_by(Site_ID) %>%
summarise(site = first(Site_ID),
observer = first(Observer),
temp = first(Temp),
avgNoise = first(Avg_Noise),
dayOfYear = first (dayOfYear),
mas = first(mas))
umf <- unmarkedFrameDS(y = as.matrix(yDat), siteCovs = as.data.frame(covs),
survey = "point", dist.breaks = c(0,25,100,200,400), unitsIn = "m")
# Fitting models.
# Half-normal, null
hnNull <- distsamp(~1~1, umf, keyfun = "halfnorm", output = "density", unitsOut = "kmsq")
#Half-normal, MAS
hnMAS <- distsamp(~mas ~1, umf, keyfun = "halfnorm", output = "density", unitsOut = "kmsq")
# Half-normal, Day of year
hnDay <- distsamp(~dayOfYear ~1, data = umf, keyfun = "halfnorm", output = "density", unitsOut = "kmsq")
# Half-normal, noise
hnNoise <- distsamp(~avgNoise ~1, data = umf, keyfun = "halfnorm", output = "density", unitsOut = "kmsq")
# Half-normal, Temp
hnTemp <- distsamp(~temp ~1, data = umf, keyfun = "halfnorm", output = "density", unitsOut = "kmsq")
haNull <- distsamp(~1 ~1, data = umf, keyfun = "hazard", output = "density", unitsOut = "kmsq")
haNoise <- distsamp(~avgNoise ~1, data = umf, keyfun = "hazard", output = "density", unitsOut = "kmsq")
haTemp <- distsamp(~temp ~1, data = umf, keyfun = "hazard", output = "density", unitsOut = "kmsq")
haDay <- distsamp(~dayOfYear ~1, data = umf, keyfun = "hazard", output = "density", unitsOut = "kmsq")
haMAS <- distsamp(~mas ~1, data = umf, keyfun = "hazard", output = "density", unitsOut = "kmsq")
fmList <- list("haNull" = haNull, "haDay" = haDay, "haNoise" = haNoise, "haMAS" = haMAS, "haTemp" = haTemp,
"hnNull" = hnNull, "hnDay" = hnDay, "hnNoise" = hnNoise, "hnMAS" = hnMAS, "hnTemp" = hnTemp)
aictab(cand.set = fmList, second.ord = T, sort = T)
distanceCovPred <- matrix(nrow = 24, ncol = 7) #create a matrix for output
colnames(distanceCovPred) <- c("sigma", "ea", "er", "p","lowerCI", "upperCI", "Day") #give the columns names
for (i in 1:24) { #this loops through 24 days of the year to generate estimates of p-hat
distanceCovPred[[i,1]] <- backTransform(linearComb(hnDay['det'], c(1, i+157)))@estimate # this is a kludge to index days (i.e., lowest value = day 158)
distanceCovPred[[i,2]] <- 2*pi * integrate(grhn, 0, 400, sigma=distanceCovPred[[i,1]])$value # effective area
distanceCovPred[[i,3]] <- sqrt(distanceCovPred[[i,2]] / pi) # effective radius
distanceCovPred[[i,4]] <- distanceCovPred[[i,2]] / (pi*400^2) #detection probability
getPcovsL <- function(hnDay) { #this defines the function that 'parboot' will use
sig <- backTransform(linearComb(hnDay['det'], c(1, i+157)))@estimate
ea <- 2*pi * integrate(grhn, 0, 400, sigma=sig)$value # effective area
er <- sqrt(ea / pi) # effective radius
p <- ea / (pi*400^2) #detection probability
out <- c(p = p , er = er)
} #following lines are for pulling out upper and lower 95% Ci from bootstrap; parallel process didn't work, so had to set cores = 1
distanceCovPred[[i,5]] <- quantile(parboot(hnDay,getPcovsL, nsim = 100, report = 100, ncores = 1)@t.star[,1],probs = 0.025)
distanceCovPred[[i,6]] <- quantile(parboot(hnDay,getPcovsL, nsim = 100, report = 100, ncores = 1)@t.star[,1], probs = 0.975)
}
distanceCovPred[,7]<- seq(158, 181, 1)
ggplot(data = as.data.frame(distanceCovPred), aes(x = Day, y = p)) +
geom_ribbon(aes(ymin = lowerCI, ymax = upperCI), fill = "grey80", alpha = 0.5) +
geom_line(aes(x = Day, y = p), color = "black") +
xlab("Day of year") + ylab ("Probability of detection") +
theme_bw()
## Incorporating removal models
encounters <-
surveyData %>%
group_by(Site_ID) %>%
mutate(interval1 = ifelse(firstInterval == 1 & Sex == "Male" & firstDet == "Singing", 1, 0),
interval2 = ifelse(firstInterval == 2 & Sex == "Male" & firstDet == "Singing", 1, 0),
interval3 = ifelse(firstInterval == 3 & Sex == "Male" & firstDet == "Singing", 1, 0),
interval4 = ifelse(firstInterval == 4 & Sex == "Male" & firstDet == "Singing", 1, 0)) %>%
select(Site_ID, interval1, interval2, interval3, interval4, Sex, firstDet) %>%
group_by(Site_ID) %>%
summarise(interval1 = sum(interval1), # this model wants summed # of birds per interval
interval2 = sum(interval2), # and has to match the distance data
interval3 = sum(interval3),
interval4 = sum(interval4))
yRemoval <- matrix(nrow = 214, ncol = 4)
rownames(yRemoval) <- encounters$Site_ID
yRemoval <- cbind(encounters[,2:5])
yRemoval[is.na(yRemoval)] <- 0
## Create a new variable called 'distance', which translates the distance_band information into the midpoint of the distance.
## Then remove all other variables.
## THIS INCLUDES ONLY SINGING MALES ##
distsRemoval <-
surveyData %>%
group_by(Site_ID) %>%
mutate(distance = ifelse(distanceBand == 1, 12.5,
ifelse(distanceBand == 2, 61,
ifelse(distanceBand == 3, 150,
ifelse(distanceBand == 4, 300, NA))))) %>%
select(Site_ID, distance, Sex, firstDet) %>%
filter(!is.na(distance), firstDet == "Singing", Sex == "Male")
## Note here that we need the "as.data.frame" argument because 'dists' is a tidyverse tibble,
## and unmarked doesn't seem to like tibbles. This forces it into a standard R data frame.
yDatDistance <- formatDistData(distData = as.data.frame(distsRemoval), distCol = "distance", transectNameCol = "Site_ID",
dist.breaks = c(0,25,100,200,400))
umfDR <- unmarkedFrameGDR(yDistance = as.matrix(yDatDistance), yRemoval = as.matrix(yRemoval), numPrimary = 1,
siteCovs = covs, dist.breaks = c(0,25,100,200,400), unitsIn = "m")
drNull <- gdistremoval(lambdaformula = ~1, phiformula = ~1, removalformula = ~1,
distanceformula = ~1, data = umfDR, keyfun = "halfnorm",
output = "density", unitsOut = "kmsq", mixture = "ZIP")
removalFrame <- unmarkedFrameMPois(y = yRemoval, siteCovs = covs, type = "removal")
removalNull <- multinomPois(~1 ~1, data = removalFrame)
removalDay <- multinomPois(~dayOfYear ~1, data = removalFrame)
removalTemp <- multinomPois(~temp ~1, data = removalFrame)
removalNoise <- multinomPois(~avgNoise ~1, data = removalFrame)
removalMAS <- multinomPois (~mas ~1, data = removalFrame)
fmRemovalList <- list("removalNull" = removalNull, "removalDay" = removalDay, "removalTemp" = removalTemp,
"removalNoise" = removalNoise, "removalMAS" = removalMAS)
aictab(cand.set = fmRemovalList, second.ord = T, sort = T)
summary(removalDay)
removalDaySTAN <- stan_multinomPois(~scale(dayOfYear) ~1, removalFrame, chains=3, iter=300, cores = 3)
removalDaySTAN
removalDaySTANframe <- plot_effects(removalDaySTAN, "det")
ggplot(data = removalDaySTANframe$data, aes(x = covariate, y = mn)) +
geom_ribbon(aes(ymin = lower, ymax = upper), fill = "grey80", alpha = 0.5) +
geom_line(aes(x = covariate, y = mn), color = "black") +
xlab("Day of year") + ylab ("Availability for detection") +
theme_bw()
surveyData %>%
filter(!is.na(Sex)) %>%
select(Sex, Lark_ID, Interval_1, Interval_2, Interval_3, Interval_4) %>%
pivot_longer(., cols = 3:6, names_to = "interval", values_to = "detection") %>%
group_by(Lark_ID, Sex) %>%
filter(!is.na(detection)) %>%
summarise(firstDet = first(detection)) %>%
group_by(Sex, firstDet) %>%
summarise(numDetected = n())
library(knitr)
surveyData %>%
filter(!is.na(distanceBand)) %>%
group_by(distanceBand) %>%
summarise(count = n()) %>%
ggplot(data = ., aes(x = distanceBand, y = count)) + geom_col() +
geom_text(aes(label = count), vjust = 1.5, colour = "white") +
labs(x = "Distance band", y = "No. larks detected")
View(surveyData)
# Summarize and visualize distance of detections
surveyData %>%
filter(!is.na(distanceBand)) %>%
group_by(distanceBand) %>%
summarise(count = n()) %>%
ggplot(data = ., aes(x = distanceBand, y = count, fill = firstDet)) + geom_col() +
geom_text(aes(label = count), vjust = 1.5, colour = "white") +
labs(x = "Distance band", y = "No. larks detected")
# Summarize and visualize distance of detections
surveyData %>%
filter(!is.na(distanceBand)) %>%
group_by(distanceBand, firstDet) %>%
summarise(count = n()) %>%
ggplot(data = ., aes(x = distanceBand, y = count, fill = firstDet)) + geom_col() +
geom_text(aes(label = count), vjust = 1.5, colour = "white") +
labs(x = "Distance band", y = "No. larks detected")
surveyData %>%
filter(!is.na(distanceBand)) %>%
group_by(distanceBand, firstDet) %>%
summarise(count = n()) %>%
ggplot(data = ., aes(x = distanceBand, y = count, fill = firstDet)) + geom_col() +
geom_text(aes(label = count), vjust = 0.5, colour = "white") +
labs(x = "Distance band", y = "No. larks detected")
surveyData %>%
filter(!is.na(distanceBand)) %>%
group_by(distanceBand, firstDet) %>%
summarise(count = n()) %>%
ggplot(data = ., aes(x = distanceBand, y = count, fill = firstDet)) + geom_col() +
geom_text(aes(label = count), vjust = -0.5, colour = "white") +
labs(x = "Distance band", y = "No. larks detected")
surveyData %>%
filter(!is.na(distanceBand)) %>%
group_by(distanceBand, firstDet) %>%
summarise(count = n()) %>%
ggplot(data = ., aes(x = distanceBand, y = count, fill = firstDet)) + geom_col() +
geom_text(aes(label = count), vjust = 0.5, colour = "white") +
labs(x = "Distance band", y = "No. larks detected")
surveyData %>%
filter(!is.na(Sex)) %>%
select(Sex, Lark_ID, Interval_1, Interval_2, Interval_3, Interval_4) %>%
pivot_longer(., cols = 3:6, names_to = "interval", values_to = "detection") %>%
group_by(Lark_ID, Sex) %>%
filter(!is.na(detection)) %>%
summarise(firstDet = first(detection)) %>%
group_by(Sex, firstDet) %>%
summarise(numDetected = n())
hist(umf, freq = TRUE, xlab = "Distance (m)", main = "Streaked Horned Lark detections 2022", cex.lab = 0.8, cex.axis = 0.8)
aictab(cand.set = fmList, second.ord = T, sort = T)
tableDistanceAIC <- aictab(cand.set = fmList, second.ord = T, sort = T)
#| warning: false
#| echo: false
#| label: tbl-distanceAIC
#| tbl-cap: Model selection results for distance-sampling analysis of Streaked Horned Lark detections during surveys in the Willamette Valley of Oregon, 2022.
tableDistanceAIC <- aictab(cand.set = fmList, second.ord = T, sort = T)
kable(as.data.frame(tableDistanceAIC), digits = 2, col.names = c("Model","No.parameters", "AICc", "Delta AICc", "Model likelihood", "AICc weight", "Log-likelihood", "Cum. weight"), caption = "Model selection results for distance-sampling analysis of Streaked Horned Lark detections during surveys in the Willamette Valley of Oregon, 2022.")
